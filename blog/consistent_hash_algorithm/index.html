<!DOCTYPE html>
<html lang="zh-cn">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0,user-scalable=0" />
<link rel="shortcut icon" href="https://sdttttt.github.io/favicon.ico" />
<title>一致性哈希算法 | SDTTTTT&#39;s Smelly fish Rotten shrimp 📓</title>
<meta name="title" content="一致性哈希算法" />
<meta name="description" content="第一代分布式系统采用的是中心化的系统，对于存贮大量数据的分布式系统来说它的缺点就是中央节点成为了整个个分布式系统的单点故障.
第二代分布式系统，节点之间通行采用的是广播，每个节点都向自己相连的所有节点进行询问，被询问的节点如果不知道这个文件在哪里,就再次进行广播&hellip;&hellip;如此往复,直至找到所需文件。请求变多就意味着会产生广播风暴，这会严重占用带宽和系统资源。
第三代分布式系统开始采用DHT(Distrbuted Hash Table)，也就是一致性HASH算法.
算法背景 一致性 HASH 算法在 1997 年由麻省理工学院的 Karger 等人在解决分布式 Cache 中提出的,设计目标是为了 解决因特网中的热点(Hot spot)问题,初衷和 CARP 十分类似。一致性 HASH 修正了 CARP 使用的简单哈希 算法带来的问题,使得 DHT 可以在 P2P 环境中真正得到应用。
但现在一致性 hash 算法在分布式系统中也得到了广泛应用,研究过 memcached 缓存数据库的人都知道, memcached 服务器端本身不提供分布式 Cache 的一致性,而是由客户端来提供,具体在计算一致性 HASH 时采用如下步骤:
  首先求出 memcached 服务器(节点)的哈希值,并将其配置到 0 ~ 2^32 的圆(continuum)上。
  然后采用同样的方法求出存储数据的键的哈希值,并映射到相同的圆上。
  然后从数据映射到的位置开始顺时针查找,将数据保存到找到的第一个服务器上。如果超过 2^32 仍然找不到服务器,就会保存到第一台 memcached 服务器上。
  从上图的状态中添加一台 memcached 服务器。余数分布式算法由于保存键的服务器会发生巨大变化 而影响缓存的命中率,但一致性Hashing 中,只有在圆(continuum)上增加服务器的地点逆时针方向 的第一台服务器上的键会受到影响。
性质 因为考虑到整个系统的节点数量是动态的，每时每刻有新节点加入和旧节点的失效。 在这类情况下依然要保证系统的可用性，这是值得思考的，尤其是在设计分布式缓存系统的时候。 如果不采用一致性HASH算法, 客户端在计算数据的 hash 时往往要重新计算(通常这个 Hash 算法和系统中的节点数量有关), 由于 Hash 值已经改变，所以很有可能找不到在整个系统中所对应的节点，导致不可用。所以一致性HASH算法，在分布式系统中十分重要。" />
<meta name="keywords" content="distrbuted system," />


<meta property="og:title" content="一致性哈希算法" />
<meta property="og:description" content="第一代分布式系统采用的是中心化的系统，对于存贮大量数据的分布式系统来说它的缺点就是中央节点成为了整个个分布式系统的单点故障.
第二代分布式系统，节点之间通行采用的是广播，每个节点都向自己相连的所有节点进行询问，被询问的节点如果不知道这个文件在哪里,就再次进行广播&hellip;&hellip;如此往复,直至找到所需文件。请求变多就意味着会产生广播风暴，这会严重占用带宽和系统资源。
第三代分布式系统开始采用DHT(Distrbuted Hash Table)，也就是一致性HASH算法.
算法背景 一致性 HASH 算法在 1997 年由麻省理工学院的 Karger 等人在解决分布式 Cache 中提出的,设计目标是为了 解决因特网中的热点(Hot spot)问题,初衷和 CARP 十分类似。一致性 HASH 修正了 CARP 使用的简单哈希 算法带来的问题,使得 DHT 可以在 P2P 环境中真正得到应用。
但现在一致性 hash 算法在分布式系统中也得到了广泛应用,研究过 memcached 缓存数据库的人都知道, memcached 服务器端本身不提供分布式 Cache 的一致性,而是由客户端来提供,具体在计算一致性 HASH 时采用如下步骤:
  首先求出 memcached 服务器(节点)的哈希值,并将其配置到 0 ~ 2^32 的圆(continuum)上。
  然后采用同样的方法求出存储数据的键的哈希值,并映射到相同的圆上。
  然后从数据映射到的位置开始顺时针查找,将数据保存到找到的第一个服务器上。如果超过 2^32 仍然找不到服务器,就会保存到第一台 memcached 服务器上。
  从上图的状态中添加一台 memcached 服务器。余数分布式算法由于保存键的服务器会发生巨大变化 而影响缓存的命中率,但一致性Hashing 中,只有在圆(continuum)上增加服务器的地点逆时针方向 的第一台服务器上的键会受到影响。
性质 因为考虑到整个系统的节点数量是动态的，每时每刻有新节点加入和旧节点的失效。 在这类情况下依然要保证系统的可用性，这是值得思考的，尤其是在设计分布式缓存系统的时候。 如果不采用一致性HASH算法, 客户端在计算数据的 hash 时往往要重新计算(通常这个 Hash 算法和系统中的节点数量有关), 由于 Hash 值已经改变，所以很有可能找不到在整个系统中所对应的节点，导致不可用。所以一致性HASH算法，在分布式系统中十分重要。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sdttttt.github.io/blog/consistent_hash_algorithm/" />
<meta property="og:image" content="https://sdttttt.github.io/static/me.jpg"/>
<meta property="article:published_time" content="2020-04-27T10:05:10+08:00" />
<meta property="article:modified_time" content="2020-04-27T10:05:10+08:00" /><meta property="og:site_name" content="SDTTTTT" />



<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://sdttttt.github.io/static/me.jpg"/>

<meta name="twitter:title" content="一致性哈希算法"/>
<meta name="twitter:description" content="第一代分布式系统采用的是中心化的系统，对于存贮大量数据的分布式系统来说它的缺点就是中央节点成为了整个个分布式系统的单点故障.
第二代分布式系统，节点之间通行采用的是广播，每个节点都向自己相连的所有节点进行询问，被询问的节点如果不知道这个文件在哪里,就再次进行广播&hellip;&hellip;如此往复,直至找到所需文件。请求变多就意味着会产生广播风暴，这会严重占用带宽和系统资源。
第三代分布式系统开始采用DHT(Distrbuted Hash Table)，也就是一致性HASH算法.
算法背景 一致性 HASH 算法在 1997 年由麻省理工学院的 Karger 等人在解决分布式 Cache 中提出的,设计目标是为了 解决因特网中的热点(Hot spot)问题,初衷和 CARP 十分类似。一致性 HASH 修正了 CARP 使用的简单哈希 算法带来的问题,使得 DHT 可以在 P2P 环境中真正得到应用。
但现在一致性 hash 算法在分布式系统中也得到了广泛应用,研究过 memcached 缓存数据库的人都知道, memcached 服务器端本身不提供分布式 Cache 的一致性,而是由客户端来提供,具体在计算一致性 HASH 时采用如下步骤:
  首先求出 memcached 服务器(节点)的哈希值,并将其配置到 0 ~ 2^32 的圆(continuum)上。
  然后采用同样的方法求出存储数据的键的哈希值,并映射到相同的圆上。
  然后从数据映射到的位置开始顺时针查找,将数据保存到找到的第一个服务器上。如果超过 2^32 仍然找不到服务器,就会保存到第一台 memcached 服务器上。
  从上图的状态中添加一台 memcached 服务器。余数分布式算法由于保存键的服务器会发生巨大变化 而影响缓存的命中率,但一致性Hashing 中,只有在圆(continuum)上增加服务器的地点逆时针方向 的第一台服务器上的键会受到影响。
性质 因为考虑到整个系统的节点数量是动态的，每时每刻有新节点加入和旧节点的失效。 在这类情况下依然要保证系统的可用性，这是值得思考的，尤其是在设计分布式缓存系统的时候。 如果不采用一致性HASH算法, 客户端在计算数据的 hash 时往往要重新计算(通常这个 Hash 算法和系统中的节点数量有关), 由于 Hash 值已经改变，所以很有可能找不到在整个系统中所对应的节点，导致不可用。所以一致性HASH算法，在分布式系统中十分重要。"/>



<meta itemprop="name" content="一致性哈希算法">
<meta itemprop="description" content="第一代分布式系统采用的是中心化的系统，对于存贮大量数据的分布式系统来说它的缺点就是中央节点成为了整个个分布式系统的单点故障.
第二代分布式系统，节点之间通行采用的是广播，每个节点都向自己相连的所有节点进行询问，被询问的节点如果不知道这个文件在哪里,就再次进行广播&hellip;&hellip;如此往复,直至找到所需文件。请求变多就意味着会产生广播风暴，这会严重占用带宽和系统资源。
第三代分布式系统开始采用DHT(Distrbuted Hash Table)，也就是一致性HASH算法.
算法背景 一致性 HASH 算法在 1997 年由麻省理工学院的 Karger 等人在解决分布式 Cache 中提出的,设计目标是为了 解决因特网中的热点(Hot spot)问题,初衷和 CARP 十分类似。一致性 HASH 修正了 CARP 使用的简单哈希 算法带来的问题,使得 DHT 可以在 P2P 环境中真正得到应用。
但现在一致性 hash 算法在分布式系统中也得到了广泛应用,研究过 memcached 缓存数据库的人都知道, memcached 服务器端本身不提供分布式 Cache 的一致性,而是由客户端来提供,具体在计算一致性 HASH 时采用如下步骤:
  首先求出 memcached 服务器(节点)的哈希值,并将其配置到 0 ~ 2^32 的圆(continuum)上。
  然后采用同样的方法求出存储数据的键的哈希值,并映射到相同的圆上。
  然后从数据映射到的位置开始顺时针查找,将数据保存到找到的第一个服务器上。如果超过 2^32 仍然找不到服务器,就会保存到第一台 memcached 服务器上。
  从上图的状态中添加一台 memcached 服务器。余数分布式算法由于保存键的服务器会发生巨大变化 而影响缓存的命中率,但一致性Hashing 中,只有在圆(continuum)上增加服务器的地点逆时针方向 的第一台服务器上的键会受到影响。
性质 因为考虑到整个系统的节点数量是动态的，每时每刻有新节点加入和旧节点的失效。 在这类情况下依然要保证系统的可用性，这是值得思考的，尤其是在设计分布式缓存系统的时候。 如果不采用一致性HASH算法, 客户端在计算数据的 hash 时往往要重新计算(通常这个 Hash 算法和系统中的节点数量有关), 由于 Hash 值已经改变，所以很有可能找不到在整个系统中所对应的节点，导致不可用。所以一致性HASH算法，在分布式系统中十分重要。">
<meta itemprop="datePublished" content="2020-04-27T10:05:10&#43;08:00" />
<meta itemprop="dateModified" content="2020-04-27T10:05:10&#43;08:00" />
<meta itemprop="wordCount" content="134">
<meta itemprop="image" content="https://sdttttt.github.io/static/me.jpg"/>



<meta itemprop="keywords" content="distrbuted system," />
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  body {
    font-family: Verdana, sans-serif;
    margin: auto;
    padding: 20px;
    max-width: 720px;
    text-align: left;
    background-color: white;
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: #444;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6,
  strong,
  b {
    color: #222;
  }

  a {
    color: #3273dc;
  }

  .title {
    text-decoration: none;
    border: 0;
  }

  .title span {
    font-weight: 400;
  }

  nav a {
    margin-right: 10px;
  }

  textarea {
    width: 100%;
    font-size: 16px;
  }

  input {
    font-size: 16px;
  }

  content {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  img {
    max-width: 100%;
  }

  code {
    padding: 2px 5px;
    background-color: #eee;
  }

  pre code {
    border-left: 1px solid #999;
    color: #555;
    display: block;
    padding: 10px;
    white-space: pre-wrap;
  }

  blockquote {
    border-left: 1px solid #999;
    color: #555;
    padding-left: 10px;
    font-style: italic;
  }

  footer {
    padding: 25px;
    text-align: center;
  }

  .helptext {
    color: #777;
    font-size: small;
  }

  .errorlist {
    color: #eba613;
    font-size: small;
  }

</style>

</head>

<body>
  <header><a href="/" class="title">
  <h2>SDTTTTT&#39;s Smelly fish Rotten shrimp 📓</h2>
</a>
<nav><a href="/">Home</a>


<a href="/blog">Blog</a>

</nav>
</header>
  <main>

<h1>一致性哈希算法</h1>
<hr />
<p>
  <i>
    <time datetime='2020-04-27' pubdate>
      27 Apr, 2020
    </time>
  </i>

    <img align="right" width="60px" height="60px" src="/me.jpg" />
 
  <h5 align="right">Author: <a href="https://github.com/sdttttt">sdttttt</a></h5>
</p>


<content>
  <p>第一代分布式系统采用的是中心化的系统，对于存贮大量数据的分布式系统来说它的缺点就是<code>中央节点</code>成为了整个个分布式系统的<code>单点故障</code>.</p>
<p>第二代分布式系统，节点之间通行采用的是<code>广播</code>，每个节点都向自己相连的<code>所有节点</code>进行询问，被询问的节点如果不知道这个文件在哪里,就再次进行<code>广播</code>&hellip;&hellip;如此往复,直至找到所需文件。请求变多就意味着会产生<code>广播风暴</code>，这会严重占用带宽和系统资源。</p>
<p>第三代分布式系统开始采用<code>DHT</code>(Distrbuted Hash Table)，也就是<code>一致性HASH算法</code>.</p>
<h2 id="算法背景">算法背景</h2>
<p>一致性 HASH 算法在 1997 年由麻省理工学院的 Karger 等人在解决分布式 Cache 中提出的,设计目标是为了
解决因特网中的热点(Hot spot)问题,初衷和 CARP 十分类似。一致性 HASH 修正了 CARP 使用的简单哈希
算法带来的问题,使得 DHT 可以在 P2P 环境中真正得到应用。</p>
<p>但现在一致性 hash 算法在分布式系统中也得到了广泛应用,研究过 <code>memcached</code> 缓存数据库的人都知道,
<code>memcached</code> 服务器端本身不提供分布式 <code>Cache</code> 的一致性,而是由客户端来提供,具体在计算一致性 HASH 时采用如下步骤:</p>
<ul>
<li>
<p>首先求出 memcached 服务器(节点)的哈希值,并将其配置到 <code>0 ~ 2^32</code> 的圆(continuum)上。</p>
</li>
<li>
<p>然后采用同样的方法求出存储数据的键的哈希值,并映射到相同的圆上。</p>
</li>
<li>
<p>然后从数据映射到的位置开始顺时针查找,将数据保存到找到的第一个服务器上。如果超过 2^32 仍然找不到服务器,就会保存到第一台 memcached 服务器上。</p>
</li>
</ul>
<p><img src="/ConsistentHashAlgorithm/1.png" alt=""></p>
<p>从上图的状态中添加一台 memcached 服务器。余数分布式算法由于保存键的服务器会发生巨大变化
而影响缓存的命中率,但<code>一致性Hashing</code> 中,只有在圆(continuum)上增加服务器的地点逆时针方向
的第一台服务器上的键会受到影响。</p>
<h2 id="性质">性质</h2>
<p>因为考虑到整个系统的节点数量是动态的，每时每刻有新节点加入和旧节点的失效。
在这类情况下依然要保证系统的可用性，这是值得思考的，尤其是在设计分布式缓存系统的时候。
如果不采用<code>一致性HASH算法</code>, 客户端在计算数据的 hash 时往往要重新计算(通常这个 Hash 算法和系统中的节点数量有关),
由于 Hash 值已经改变，所以很有可能找不到在整个系统中所对应的节点，导致不可用。所以<code>一致性HASH算法</code>，在分布式系统中十分重要。</p>
<p>良好的<code>一致性HASH算法</code>需要满足一下特点：</p>
<p><strong>平衡性(Balance)</strong></p>
<p>平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去,这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。</p>
<p><strong>单调性(Monotonicity)</strong></p>
<p>单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中,又有新的缓冲区加入到系统中,那么哈 希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去,而不会被映射到旧的缓冲集合中的 其他缓冲区。简单的哈希算法往往不能满足单调性的要求,如最简单的线性哈希:x = (ax + b) mod (P), 在上式中,P 表示全部缓冲的大小。不难看出,当缓冲大小发生变化时(从 P1 到 P2),原来所有的哈希结果 均会发生变化,从而不满足单调性的要求。哈希结果的变化意味着当缓冲空间发生变化时,所有的映射关 系需要在系统内全部更新。而在 P2P 系统内,缓冲的变化等价于 Peer 加入或退出系统,这一情况在 P2P 系 统中会频繁发生,因此会带来极大计算和传输负荷。单调性就是要求哈希算法能够应对这种情况。</p>
<p><strong>分散性(Spread)</strong></p>
<p>在分布式环境中,终端有可能看不到所有的缓冲,而是只能看到其中的一部分。当终端希望通过哈希过程 将内容映射到缓冲上时,由于不同终端所见的缓冲范围有可能不同,从而导致哈希的结果不一致,最终的 结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的,因为它导致相同内 容被存储到不同缓冲中去,降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈 希算法应能够尽量避免不一致的情况发生,也就是尽量降低分散性。</p>
<p><strong>负载(Load)</strong></p>
<p>负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区 中,那么对于一个特定的缓冲区而言,也可能被不同的用户映射为不同的内容。与分散性一样,这种情况 也是应当避免的,因此好的哈希算法应能够尽量降低缓冲的负荷。</p>
<p><strong>平滑性(Smoothness)</strong></p>
<p>平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的。</p>
<h2 id="虚拟节点">虚拟节点</h2>
<p>假设在圆上<code>Node A</code>和<code>Node B</code>距离过近，按照以上的环形一致 HASH 算法就会发生两个节点所拥有的数据数量不一致的问题。</p>
<p><img src="/ConsistentHashAlgorithm/2.png" alt=""></p>
<p>为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算<code>多个哈希</code>，每个计算结果位置都放置一个此<code>服务节点</code>，称为虚拟节点。具体做法可以在服务器 ip 或主机名的后面增加编号来实现。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点：</p>
<p>同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到 Node A 上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为 32 甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。</p>

</content>
<p>
  
  <a href="https://sdttttt.github.io/tags/distrbuted-system/">#distrbuted system</a>
  
</p>

  </main>
  <footer>
<script async src="https://comments.app/js/widget.js?3" data-comments-app-website="16l5H9Ux" data-limit="5"></script>
<img src="github.png" /></footer>

    
</body>

</html>
